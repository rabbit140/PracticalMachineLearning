---
title: "Predicting exercise type using classification models"
author: "Przemyslaw Zientala"
date: "28 February 2016"
output: html_document
---

#Introduction
The aim of this project is to predict (*classify*) the manner in which subjects of [a study](http://groupware.les.inf.puc-rio.br/har) exercised. To accomplish this, a model selection procedure was utilised and the best model was selected according to its accuracy.

#Background
Using devices such as *Jawbone Up*, *Nike FuelBand*, and *Fitbit* it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal was to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

#Data preparation

###Data cleaning
Since both datasets contained many redundant columns containing little to no information (either "NA" vlaues or empty cells), these were removed to make the analysis and model fitting easier.

```{r, cache=FALSE,results='hide', warning=FALSE, message=FALSE, error=FALSE}
library(rattle)
library(caret)
library(randomForest)
```
```{r,cache=TRUE}
#Load the data
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")

#Select complete columns
na.prop <- function(x){sum(is.na(x))/length(x)*100}
cols_prop <- sapply(training, na.prop)
#Threshold of incompleteness: 10%
col_names <- names(cols_prop[(cols_prop < 10)])

#Check number of "NA" values
sum(is.na(training[,col_names]))

training <- training[,col_names]
```

As it can be seen, there are no "NA" values left, which is a good sign.

Now, let's remove all empty values:
```{r, cache=TRUE}
#Select columns with no empty values (not the same as NA!)
prop.empty <- function(x){sum(x == "")/length(x)*100}
cols_prop <- sapply(training, prop.empty)
#Threshold of incompleteness: 10%
col_names <- names(cols_prop[cols_prop < 10])

#Check the number of empty cells
sum((sapply(training[,col_names], prop.empty) > 0))
training <- training[,col_names]
```

Again, there are no empty values left, so let's proceed to clean testing dataset using the same approach:

```{r, cache=TRUE}
#Select complete columns
na.prop <- function(x){sum(is.na(x))/length(x)*100}
cols_prop <- sapply(testing, na.prop)
col_names <- names(cols_prop[(cols_prop < 10)])

#Check number of "NA" values
sum(is.na(testing[,col_names]))

testing <- testing[,col_names]

#Select columns with no empty values (not the same as NA!)
prop.empty <- function(x){sum(x == "")/length(x)*100}
cols_prop <- sapply(testing, prop.empty)
col_names <- names(cols_prop[cols_prop < 10])

#Check the number of empty cells
sum((sapply(testing[,col_names], prop.empty) > 0))
testing <- testing[,col_names]

#Remove "X" col
training <- training[,-c(1:5)]
testing <- testing[,-c(1:5)]
```

###Variable selection and splitting the data

Let's now remove any near-zero variance predictors:
```{r,cache=TRUE}
nearZeroVar(training)
nearZeroVar(testing)
training <- training[,-nearZeroVar(training)]
testing <- testing[,-nearZeroVar(testing)]
```

The first column in both datasets was identified as being a near-zero variance predictor, which, even purely by visual inspection, looks correct. Therefore, it was removed and the remaining predictors were used.

Now, all the data is ready to be split into training and validation sets. To ensure reproducibility, the seed was set to 1:
```{r,cache=TRUE}
#Split data into train and test sets (66:34 split)
set.seed(1)
train_rows <- sample(1:nrow(training), size = 2/3*nrow(training))
train_data <- training[train_rows,]
test_data <- training[which(1:nrow(training) %in% train_rows == FALSE),]
```

#Model selection
Since there are
```{r}
p = ncol(train_data) - 1
```

variables, it would be unreasonable to adopt the best subset selection approach, as we would have to choose the best combination out of

```{r}
2^p
```

total combinations, which would be extremely expensive computationally, depending on the type of model fitted. Forward or backward stepwise selection could be considered less computationally expensive, although, with this number of variables, it would still take a long time to find the best model. Moreover, the "stepAIC" function is not compatible with models trained using the "caret" package and there are very limited options within the package to perform variable selection. Any regression method, such as ridge or lasso, cannot be applied since the task is to classify discrete values (classes A to E) and not continuous ones. Thus, we are left with classification trees (with bagging and boosting), random forest, or with LDA/QDA. Let's select the type of model trained on all available variables and choose the best one based on overall accuracy.

###Classification tree
```{r,cache=TRUE}
class.tree <- train(classe~., data = train_data, method = "rpart")
confusionMatrix(test_data$classe, predict(class.tree, test_data))
```
The overall accuracy is quite low (~50%) and the Kappa statistic is ~37%. Let's tryfittin a random forest:

###Random forest
```{r,cache=TRUE}
forest.fit <- randomForest(classe~., data = train_data)
confusionMatrix(test_data$classe, predict(forest.fit, test_data))
```

Clearly, this model is extremely accurate as assessed on test data, with overall accuracy and Kappa statistic both being close to 100%.

###Boosting
Unfortunately, boosting could not be used due to computational limitations of the CPU, and after half an hour the execution of code was stopped.

###LDA
Let's try fitting LDA model:

```{r,cache=TRUE}
lda.fit <- train(classe~., data = train_data, method = "lda")
confusionMatrix(test_data$classe, predict(lda.fit, test_data))
```

The accuracy seems to have improved by over 20% and Kappa statistic raised by almost 30% in comparison to the classification tree.

###QDA
Now, fit a QDA model:
```{r,cache=TRUE}
qda.fit <- train(classe~., data = train_data, method = "qda")
confusionMatrix(test_data$classe, predict(qda.fit, test_data))
```

Fitting a QDA model resulted in further 20% accuracy improvement and a roughly 25% increase in Kappa statistic, giving a total of 40% better accuracy and 50% higher Kappa statistic than a classification tree. For comparison purposes, it is better to look at Kappa statistic as it describes how closely the examples classified by a model match the *ground truth* data while controlling for the accuracy of a random classifier as measured by the *expected accuracy* (http://stats.stackexchange.com/questions/82162/kappa-statistic-in-plain-english). However, QDA did not improve over the random forest. Thus, for predicting exercise types random forest model will be used.

#Prediction
Let's now predict the cases for the "testing" dataset and evaluate the performance of the model by reporting the grade from the fial Quiz:
```{r}
predict(forest.fit, testing)
```

This submission gained a 100% mark, meaning that the classifier was indeed very accurate on unseen data.

#Conclusion
To summarise, it is clearly important to consider various type of models as their accuracy may vary widely depending on the data. Moreover, an assessment of computational cost needs to be done so as to avoid fitting models for a very long time only to find out that they perform poorly.